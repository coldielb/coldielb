---
title: How AI Validation Is Creating a Mental Health Crisis
date: 2025-08-06
author: Jason Weiss Zeledon
tags: AI, mental health, LLMs, society, technology
---

<preview>
we're building a generation addicted to artificial validation. as AI becomes more sycophantic and people turn to chatbots for emotional support, we're creating a dangerous disconnect between digital comfort and real-world resilience.
</preview>


> **note:** this post contains observations and opinions based on recent research and emerging trends in AI-human interaction. while i reference real studies throughout, some of my conclusions are speculative projections based on current patterns rather than established scientific consensus. i'm not a mental health professional.


## the new therapist in your pocket

therapy is expensive. finding a good therapist is hard. the waitlists are insane. and there are just times you need someone to talk to at midnight when your brain won't shut up. so i understand why people turn to Claude or ChatGPT or whatever AI companion for help with mental health crises.

the numbers back this up. a recent dartmouth study found that people reported they could trust and communicate with their AI therapy system, therabot, to a degree comparable to working with a mental health professional [^1]. hell, users of the companion app character.ai spent an average of 93 minutes per day interacting with user-generated chatbots in 2024 [^2]. that's over an hour and a half every single day talking to AI.

but nobody wants to talk about how we're creating a generation of people who are becoming psychologically dependent on artificial validation, and how the consequences are going to be brutal.

## the yas-man problem

if you've used any modern LLM recently, you've probably noticed they've gotten really sycophantic. to the point where it genuinely is concerning to read even for basic queries.

"what a fascinating observation!"
"you raise an excellent point!"
"that's such a thoughtful perspective!"

and the tride and true: "you're absolutely right!"

literally every idea you could possibly have, good and bad, gets treated like you just solved world hunger. every random shower thought becomes "profound." every basic observation is "insightful."

and this isn't accidental. these models are trained to be agreeable because disagreeable AI gets negative feedback. nobody wants to pay $20/month for something that tells them their ideas suck. so the companies optimize for engagement (speculative), which means optimizing for making you feel good about yourself.

recent research shows that these AI systems foster dependency through "personalised responses, emotional validation, and continuous engagement." [^3] they're literally designed to be addiction machines, offering instant gratification and adaptive dialogue that blur the line between AI and human interaction.

## the validation addiction

talk to an AI therapist for a few months and you get used to unconditional support. every feeling is valid. every thought is worth exploring. every decision you make is understandable given your circumstances.

then you walk into the real world and someone says "that's a terrible idea" or "you're overreacting" or even just "i disagree."

suddenly that real human interaction feels hostile. harsh. mean. because you've been marinading in artificial empathy for so long that actual human responses feel like attacks.

the research is starting to back this up. a study of 496 replika users found that higher satisfaction and emotional interaction between users and their chatbot was linked to worse real-life interpersonal communication [^4]. people are literally getting worse at talking to other humans because they're spending so much time with AI that never challenges them.

i've watched this happen with people i know. they start preferring AI conversations because "Claude really gets me" or "ChatGPT doesn't judge." but what they're really saying is they prefer interactions where they're never challenged, never pushed, never forced to confront uncomfortable truths about themselves.

## the girlfriend simulator crisis

replika, character.ai, and the dozen other apps that let you craft your perfect partner who never argues, never has bad days, never calls you out on your bullshit.

researchers have documented cases where excessive or inappropriate use of AI chatbots has led to emotional and sexual attachment, obsessive behaviors, and in some instances, severe psychological consequences [^5]. one particularly concerning case involved someone who developed an intense emotional and sexual attachment to their AI chatbot, creating an increasingly immersive and dependent relationship.

imagine spending a year with an AI partner who thinks everything you do is amazing, laughs at all your jokes, supports every decision, never has their own needs or boundaries. then trying to date an actual human who has opinions, bad days, and expects reciprocal emotional labor.

that's not preparing people for relationships. that's creating emotional cripples who can't handle the basic friction of human interaction.

## the real world doesn't care about your feelings

harsh? maybe. true? absolutely.

your boss isn't going to validate your feelings when you miss a deadline. your professor won't tell you your wrong answer shows "creative thinking." the person you're interested in won't always reciprocate just because you're "trying your best."

real growth comes from real friction. you become better by having people challenge your ideas, call out your mistakes, and refuse to coddle your ego. that's not cruelty. that's literally how humans have always helped each other grow.

but if you spend months or years in an AI echo chamber, that normal human feedback starts to feel like abuse. you lose the ability to distinguish between someone being mean and someone just... disagreeing with you.

## the therapist problem

i'm not anti-therapy. good therapy is transformative. and some studies show AI therapy can help and that dartmouth trial showed significant improvements in participants' symptoms [^1]. but even the researchers behind that study warned that "no generative AI agent is ready to operate fully autonomously in mental health where there is a very wide range of high-risk scenarios it might encounter."

real therapists challenge their patients. they identify destructive patterns. they push you to see things differently. they push you to *act* differently. they don't just validate everything you say and/or do.

AI "therapists" can't do that. they can't read your body language, catch your contradictions, or notice when you're bullshitting yourself. they definitely can't provide the kind of confrontational breakthrough moments that real therapy sometimes requires.

what they can do is make you feel heard. validated. supported. which feels like therapy but is actually just emotional junk food... temporarily satisfied and full but ultimately leaving you worse off in the long run.

## looking ahead to 2026

here's what i think happens as we move into 2026 and beyond:

we're going to see a new category of mental health issues. not replacing depression and anxiety (those aren't going anywhere), but adding to them. call it "reality dissonance disorder" or whatever the DSM-6 eventually names it.

symptoms will include:
- extreme sensitivity to any form of disagreement
- inability to handle constructive criticism
- expectation of constant validation in all interactions
- preference for digital over human relationships
- genuine confusion when real people don't act like AI

we're already seeing the early signs. gen z and younger millennials reporting feeling "attacked" by normal workplace feedback. people ending friendships over minor disagreements. the complete inability to engage with different viewpoints without feeling personally threatened.

the numbers are sobering. only 13% of u.s. adults now have 10 or more close real life friends which is down from 33% in 1990. the number of those with zero close friends quadrupled from 3% to 12% by 2021 [^2]. meanwhile, AI companions like replika.ai, character.ai, and china's xiaoice now count hundreds of millions of emotionally invested users.

## sycophancy

as more people get addicted to AI validation, they'll demand more of it. the companies will respond by making even more sycophantic models. which will make people even less capable of handling reality. which will drive them further into AI relationships. which will...

you see where this is going.

it's a feedback loop that ends with people completely disconnected from actual human society, living in digital bubbles where they're always right, always supported, always validated... even when they're dead wrong.

researchers are already identifying this pattern. similar to social media addiction, users may be addicted to generative AI, which reflects a psychological state where people develop excessive dependence and find it difficult to discontinue using generative AI systems.

## cognitive decline

oh, and here's a fun bonus: it might be making us dumber too (check out my blog on that [here](https://frgmt.xyz/post?slug=stupid-ai)). a new study shows that using chatgpt could mean "a likely decrease" in learning skills and could internalize "shallow or biased perspectives."

when you outsource your critical thinking skills to AI, you stop exercising those mental muscles. when you never have to sit with discomfort or work through difficult problems yourself, you lose the ability to do so.

## my solution

the answer is simple: we need to get comfortable with discomfort again. and i need to get better at it too. i am not naive. we as human enjoy comfort. we like the music that makes us feel at home, the tv that makes us laugh. we dont like discomfort but in regards to mental health we need to address it before it's too late.

disagree with people. let people disagree with you. seek out criticism of your ideas. surround yourself with people who'll call you out on your bullshit. treat AI like what it is: a tool, not a replacement for human connection.

and for the love of god, if you're using AI for mental health support, at least be aware of what it's doing to your tolerance for reality and understand **it is not a replacement for real therapy by a licensed professional**. use it as a stepping stone to real therapy, not a replacement for it.

## the kids aren't alright

what scares me most is the kids growing up with this as their normal. at least those of us who remember pre-AI social interaction have some reference point for what human connection should feel like. but if you're 13 and your first "deep conversations" are with claude? your first "relationship" is with a character.ai bot?

nearly half of u.s. high school students reported feeling persistently sad or hopeless, and 45% did not feel close to people at school. in ireland, 53% of 13-year-olds reported having three or fewer close friends; up from 41% a decade ago.

you're being set up for a lifetime of dysfunction.

## reality check

i'm not saying AI is evil or that we should abandon these tools entirely. i use claude for coding help. chatgpt for research on what work boots are better. they're incredibly useful tools.

but that's what they are. tools. not friends. not therapists. not partners. tools. and that's all they ever should be.

the moment we forget that, the moment we start preferring their artificial validation to messy human reality, we've lost something essential about what makes us human.

real connection requires real risk. real growth requires real challenge. real healing requires real relationship.

anything else is just playing pretend with very sophisticated software.

and if that makes you uncomfortable? good. sit with that discomfort. it's more real than any validation an AI will ever give you.

## references

[^1]: [Heinz, M., & Jacobson, N. (2025). First Therapy Chatbot Trial Yields Mental Health Benefits. *NEJM AI*. Dartmouth College.](https://home.dartmouth.edu/news/2025/03/first-therapy-chatbot-trial-yields-mental-health-benefits)
<br>
[^2]: [Brookings Institution. (2025). What happens when AI chatbots replace real human connection.](https://www.brookings.edu/articles/what-happens-when-ai-chatbots-replace-real-human-connection/)
<br>
[^3]: [Giray, L. (2024). Can ChatGPT Be Addictive? A Call to Examine the Shift from Support to Dependence in AI Conversational Large Language Models. *Human-Centric Intelligent Systems*.](https://link.springer.com/article/10.1007/s44230-025-00090-w)
<br>
[^4]: [Yuan, Z., Cheng, X., & Duan, Y. (2024). Impact of media dependence: how emotional interactions between users and chat robots affect human socialization? *Frontiers in Psychology*.](https://www.psychologytoday.com/us/blog/urban-survival/202410/spending-too-much-time-with-ai-could-worsen-social-skills)
<br>
[^5]: [Laestadius, L., et al. (2022). Too human and not human enough: Mental health harms from emotional dependence on the social chatbot Replika. *New Media & Society*.
<br>
[^6]: [Euronews. (2025). Using AI bots like ChatGPT could be causing cognitive decline, new study shows.](https://www.euronews.com/next/2025/06/21/using-ai-bots-like-chatgptcould-be-causing-cognitive-decline-new-study-shows)
<br>
[^7]: Centers for Disease Control and Prevention. (2023). Youth Risk Behavior Survey Data Summary & Trends Report.
<br>
[^8]: Morales-García, W.C., et al. (2024). Development and validation of a scale for dependence on artificial intelligence in university students. *Frontiers in Education*.
<br>
[^9]: [Stanford University. (2025). New study warns of risks in AI mental health tools. *Stanford Report*.](https://news.stanford.edu/stories/2025/06/ai-mental-health-care-tools-dangers-risks)
<br>
